library(jsonlite)
library(dplyr)
library(jsonlite)
library(dplyr)
library(utils)
library(stringi)
library(jsonlite)
library(dplyr)
library(utils)
library(stringi)
# Adds duplicate question dataset
duplicate_questions_raw <- fromJSON("stackexchange_duplicatequestions.json")
colnames(duplicate_questions_raw) <- c("sentence1", "sentence2")
set.seed(123)
duplicate_questions <- as.data.frame(duplicate_questions_raw,
row.names = c("sentence1", "sentence2"))
# Generate random ids
duplicate_questions$id <- stri_rand_strings(nrow(duplicate_questions),
50,
pattern="[0-9a-f]")
# Set is_duplicate to 1.0
duplicate_questions <- duplicate_questions
duplicate_questions <- duplicate_questions %>%
mutate(is_duplicate = 1.0) %>%
select(id, sentence1, sentence2, is_duplicate)
# Just get the first 1000 observations
write.csv(duplicate_questions, "para-train-duplicate-questions.csv",
row.names = FALSE, sep="\t")
# Make sure that csv is tab delimited
# Add merged para
para_base <- read.delim("quora-train.csv")
para_merged <- rbind(para_base, duplicate_questions)
# Make sure that csv is tab delimited
write.table(para_merged, "para-train-merged.csv",
row.names = FALSE, sep="\t")
# Adds duplicate question dataset
duplicate_questions_raw <- fromJSON("stackexchange_duplicatequestions.json")
colnames(duplicate_questions_raw) <- c("sentence1", "sentence2")
set.seed(123)
duplicate_questions <- as.data.frame(duplicate_questions_raw,
row.names = c("sentence1", "sentence2"))
# Generate random ids
duplicate_questions$id <- stri_rand_strings(nrow(duplicate_questions),
50,
pattern="[0-9a-f]")
# Set is_duplicate to 1.0
duplicate_questions <- duplicate_questions[0:50000]
# Adds duplicate question dataset
duplicate_questions_raw <- fromJSON("stackexchange_duplicatequestions.json")
colnames(duplicate_questions_raw) <- c("sentence1", "sentence2")
set.seed(123)
duplicate_questions <- as.data.frame(duplicate_questions_raw,
row.names = c("sentence1", "sentence2"))
# Generate random ids
duplicate_questions$id <- stri_rand_strings(nrow(duplicate_questions),
50,
pattern="[0-9a-f]")
# Set is_duplicate to 1.0
duplicate_questions <- duplicate_questions[0:50000,]
duplicate_questions <- duplicate_questions %>%
mutate(is_duplicate = 1.0) %>%
select(id, sentence1, sentence2, is_duplicate)
# Just get the first 1000 observations
write.csv(duplicate_questions, "para-train-duplicate-questions.csv",
row.names = FALSE, sep="\t")
View(duplicate_questions)
# Make sure that csv is tab delimited
# Add merged para
para_base <- read.delim("quora-train.csv")
para_merged <- rbind(para_base, duplicate_questions)
# Make sure that csv is tab delimited
write.table(para_merged, "para-train-merged.csv",
row.names = FALSE, sep="\t")
View(para_base)
# Make sure that csv is tab delimited
# Add merged para
para_base <- read.delim("quora-train.csv")
para_merged <- rbind(para_base, duplicate_questions)
library(jsonlite)
library(dplyr)
library(utils)
library(stringi)
# This merges the cfimdb and base files so that they exist in one csv file
csfimdb <- read.delim("ids-cfimdb-train.csv")
base <- read.delim("ids-sst-train.csv")
# Transform labels of cfimdb
csfimdb$sentiment = ifelse(csfimdb$sentiment == 1, 4, 1)
csfimdb <- csfimdb %>%
filter(sentiment == 4)
df_merged <- rbind(base, csfimdb)
df_merged <- df_merged %>%
select(id, sentence, sentiment)
# Make sure that csv is tab delimited
write.table(df_merged, "ids-sst-train-merged.csv", row.names = FALSE, sep="\t")
# Adds duplicate question dataset
duplicate_questions_raw <- fromJSON("stackexchange_duplicatequestions.json")
colnames(duplicate_questions_raw) <- c("sentence1", "sentence2")
set.seed(123)
duplicate_questions <- as.data.frame(duplicate_questions_raw,
row.names = c("sentence1", "sentence2"))
# Generate random ids
duplicate_questions$id <- stri_rand_strings(nrow(duplicate_questions),
50,
pattern="[0-9a-f]")
# Set is_duplicate to 1.0
duplicate_questions <- duplicate_questions[0:50000,]
duplicate_questions <- duplicate_questions %>%
mutate(is_duplicate = 1.0) %>%
select(id, sentence1, sentence2, is_duplicate)
# Just get the first 1000 observations
write.csv(duplicate_questions, "para-train-duplicate-questions.csv",
row.names = FALSE, sep="\t")
# Make sure that csv is tab delimited
# Add merged para
para_base <- read.delim("quora-train.csv")
para_merged <- rbind(para_base, duplicate_questions)
# Make sure that csv is tab delimited
write.table(para_merged, "para-train-merged.csv",
row.names = FALSE, sep="\t")
# Add cosine similarity loss data
sts_raw <- read.delim("sts-train.csv")
# 1 for equivalent sentences, 0 for non equivalent sentences
cos_sim <- sts_raw %>%
filter(similarity >= 4.0| similarity < 0.8) %>%
mutate(equivalence = ifelse(similarity >= 4.0, 1, -1)) %>%
select(id, sentence1, sentence2, equivalence)
# Make sure that csv is tab delimited
write.csv(cos_sim, "cos-sim-train.csv",
row.names = FALSE, sep="\t")
# Add negative rankings loss data
sts_raw <- read.delim("sts-train.csv")
neg_rankings <- sts_raw %>%
filter(similarity >= 3.5) %>%
select(id, sentence1, sentence2, similarity)
# Make sure that csv is tab delimited
write.csv(neg_rankings, "neg-rankings-train.csv",
row.names = FALSE, sep="\t")
